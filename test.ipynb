{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"logs/amos_ver2/weights/*.pt\"\n",
    "\n",
    "for w in glob.glob(path):\n",
    "    num = int(w.split(\"_\")[-1].strip(\".pt\"))\n",
    "    if num % 10 != 0:\n",
    "        os.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "class MultiNeighborLoss(_Loss):\n",
    "    def __init__(self, \n",
    "                 num_classes: int, \n",
    "                 reduction: str = \"mean\", \n",
    "                 centroid_method: str = \"mean\"):\n",
    "        super(MultiNeighborLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reduction = reduction\n",
    "        self.centroid_method = centroid_method\n",
    "        self.max_count = self.num_classes * (self.num_classes - 1) // 2\n",
    "        \n",
    "    def forward(self, probs: torch.Tensor, labels: torch.Tensor):\n",
    "        assert probs.ndim == labels.ndim == 5, \"The dimensions of probs and labels should be same and 5.\"\n",
    "        \n",
    "        delta = []\n",
    "        for i in range(probs.size(0)):\n",
    "            p_angles, l_angles = self.compute_angles(torch.sigmoid(probs[i, ...])), self.compute_angles(labels[i, ...])\n",
    "            delta.append(torch.square(p_angles - l_angles))\n",
    "        \n",
    "        delta = torch.cat(delta)\n",
    "        not_nans = ~torch.isnan(delta)\n",
    "        delta = delta[not_nans]\n",
    "        delta = delta[delta > 0]\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return torch.mean(delta)\n",
    "        \n",
    "    def compute_angles(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        angles = torch.zeros(self.max_count*self.max_count).to(t.device)\n",
    "        vectors = torch.zeros(self.max_count, 3).to(t.device)\n",
    "        centroids = torch.zeros((self.num_classes, 3)).to(t.device)\n",
    "        \n",
    "        t = torch.argmax(t, dim=0)\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            z, y, x = torch.where(t == i)\n",
    "            centroids[i] = torch.stack(self.compute_centroids(x, y, z))\n",
    "            print(centroids[i])\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(i+1, self.num_classes):\n",
    "                vectors[idx] = centroids[j] - centroids[i]\n",
    "                idx += 1\n",
    "    \n",
    "        idx = 0\n",
    "        for i in range(self.max_count):\n",
    "            m = vectors[i]\n",
    "            for j in range(i+1, self.max_count):\n",
    "                n = vectors[j]\n",
    "                angle = torch.acos(torch.dot(m, n) / (torch.norm(m) * torch.norm(n)))\n",
    "                angles[idx] = angle\n",
    "                idx += 1\n",
    "                \n",
    "        return angles\n",
    "    \n",
    "    def compute_centroids(self, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor):\n",
    "        if self.centroid_method == \"mean\":\n",
    "            return [torch.mean(x.float()), torch.mean(y.float()), torch.mean(z.float())]\n",
    "        else:\n",
    "            raise NotImplementedError(f\"The centroid method is not supported. : {self.centroid_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47.5223, 47.6678, 47.4686], device='cuda:1')\n",
      "tensor([47.5407, 47.6690, 47.4988], device='cuda:1')\n",
      "tensor([47.6857, 47.3836, 47.4982], device='cuda:1')\n",
      "tensor([47.5707, 47.4679, 47.6160], device='cuda:1')\n",
      "tensor([47.4402, 47.6460, 47.5364], device='cuda:1')\n",
      "tensor([47.5677, 47.1164, 47.4269], device='cuda:1')\n",
      "tensor([47.3891, 47.4585, 47.5519], device='cuda:1')\n",
      "tensor([47.4632, 47.5281, 47.5006], device='cuda:1')\n",
      "tensor([47.4569, 47.5409, 47.4113], device='cuda:1')\n",
      "tensor([47.5372, 47.4871, 47.5200], device='cuda:1')\n",
      "tensor([47.3665, 47.4282, 47.4687], device='cuda:1')\n",
      "tensor([47.5855, 47.3465, 47.5200], device='cuda:1')\n",
      "tensor([47.7364, 47.5262, 47.5768], device='cuda:1')\n",
      "tensor([47.3583, 47.4969, 47.3249], device='cuda:1')\n",
      "tensor([47.2896, 47.5045, 47.5268], device='cuda:1')\n",
      "tensor([47.1772, 47.6582, 47.5131], device='cuda:1')\n",
      "tensor([47.6740, 47.4874, 47.4963], device='cuda:1')\n",
      "tensor([47.3916, 47.4529, 47.4113], device='cuda:1')\n",
      "tensor([47.5216, 47.3103, 47.6167], device='cuda:1')\n",
      "tensor([47.4302, 47.5911, 47.5623], device='cuda:1')\n",
      "tensor([47.3993, 47.6134, 47.3421], device='cuda:1')\n",
      "tensor([47.5853, 47.4992, 47.6983], device='cuda:1')\n",
      "tensor([47.4060, 47.3264, 47.5406], device='cuda:1')\n",
      "tensor([47.5561, 47.3920, 47.5298], device='cuda:1')\n",
      "tensor([47.5911, 47.4942, 47.3782], device='cuda:1')\n",
      "tensor([47.4155, 47.5748, 47.3085], device='cuda:1')\n",
      "tensor([47.4890, 47.4851, 47.2674], device='cuda:1')\n",
      "tensor([47.6200, 47.7204, 47.5504], device='cuda:1')\n",
      "tensor([47.3106, 47.8462, 47.5012], device='cuda:1')\n",
      "tensor([47.6410, 47.6073, 47.7179], device='cuda:1')\n",
      "tensor([47.4863, 47.4683, 47.6810], device='cuda:1')\n",
      "tensor([47.4520, 47.2940, 47.4319], device='cuda:1')\n",
      "loss : 0.8863\n",
      "tensor([47.4561, 47.5688, 47.5455], device='cuda:1')\n",
      "tensor([47.5546, 47.3738, 47.4676], device='cuda:1')\n",
      "tensor([47.4567, 47.5243, 47.4560], device='cuda:1')\n",
      "tensor([47.4375, 47.5762, 47.3440], device='cuda:1')\n",
      "tensor([47.3394, 47.4859, 47.4406], device='cuda:1')\n",
      "tensor([47.6194, 47.5122, 47.6372], device='cuda:1')\n",
      "tensor([47.4106, 47.4000, 47.5607], device='cuda:1')\n",
      "tensor([47.5158, 47.5377, 47.4678], device='cuda:1')\n",
      "tensor([47.5794, 47.4728, 47.4588], device='cuda:1')\n",
      "tensor([47.6643, 47.5480, 47.6075], device='cuda:1')\n",
      "tensor([47.5851, 47.5461, 47.5112], device='cuda:1')\n",
      "tensor([47.3754, 47.4022, 47.7745], device='cuda:1')\n",
      "tensor([47.5890, 47.4230, 47.3914], device='cuda:1')\n",
      "tensor([47.4550, 47.6472, 47.5200], device='cuda:1')\n",
      "tensor([47.6647, 47.4571, 47.6167], device='cuda:1')\n",
      "tensor([47.4009, 47.5351, 47.2289], device='cuda:1')\n",
      "tensor([47.5637, 47.5692, 47.4742], device='cuda:1')\n",
      "tensor([47.3926, 47.5612, 47.3726], device='cuda:1')\n",
      "tensor([47.5121, 47.5959, 47.6582], device='cuda:1')\n",
      "tensor([47.6375, 47.4293, 47.5215], device='cuda:1')\n",
      "tensor([47.6294, 47.5337, 47.3544], device='cuda:1')\n",
      "tensor([47.4340, 47.4862, 47.5209], device='cuda:1')\n",
      "tensor([47.4520, 47.3724, 47.7438], device='cuda:1')\n",
      "tensor([47.4613, 47.5475, 47.3328], device='cuda:1')\n",
      "tensor([47.5229, 47.3368, 47.5303], device='cuda:1')\n",
      "tensor([47.4873, 47.5579, 47.5736], device='cuda:1')\n",
      "tensor([47.5657, 47.5390, 47.3326], device='cuda:1')\n",
      "tensor([47.5229, 47.5877, 47.3398], device='cuda:1')\n",
      "tensor([47.5047, 47.3900, 47.6389], device='cuda:1')\n",
      "tensor([47.2252, 47.2625, 47.5143], device='cuda:1')\n",
      "tensor([47.3043, 47.5941, 47.5613], device='cuda:1')\n",
      "tensor([47.6569, 47.5007, 47.6149], device='cuda:1')\n",
      "loss : 0.8448\n",
      "tensor([47.5175, 47.4613, 47.4929], device='cuda:1')\n",
      "tensor([47.7034, 47.5197, 47.5890], device='cuda:1')\n",
      "tensor([47.4843, 47.5149, 47.4951], device='cuda:1')\n",
      "tensor([47.5423, 47.4079, 47.5212], device='cuda:1')\n",
      "tensor([47.2594, 47.4500, 47.5865], device='cuda:1')\n",
      "tensor([47.4750, 47.3155, 47.4518], device='cuda:1')\n",
      "tensor([47.5251, 47.4828, 47.4947], device='cuda:1')\n",
      "tensor([47.6463, 47.4475, 47.3177], device='cuda:1')\n",
      "tensor([47.4231, 47.4939, 47.6290], device='cuda:1')\n",
      "tensor([47.5259, 47.4249, 47.5345], device='cuda:1')\n",
      "tensor([47.4047, 47.5386, 47.4618], device='cuda:1')\n",
      "tensor([47.2446, 47.6233, 47.6503], device='cuda:1')\n",
      "tensor([47.6471, 47.6949, 47.4683], device='cuda:1')\n",
      "tensor([47.4696, 47.7416, 47.3695], device='cuda:1')\n",
      "tensor([47.4264, 47.5438, 47.4252], device='cuda:1')\n",
      "tensor([47.6100, 47.6549, 47.3579], device='cuda:1')\n",
      "tensor([47.4913, 47.5002, 47.5720], device='cuda:1')\n",
      "tensor([47.4460, 47.3848, 47.4617], device='cuda:1')\n",
      "tensor([47.4734, 47.5049, 47.4118], device='cuda:1')\n",
      "tensor([47.5356, 47.4973, 47.6261], device='cuda:1')\n",
      "tensor([47.4326, 47.5676, 47.5782], device='cuda:1')\n",
      "tensor([47.5660, 47.2809, 47.3853], device='cuda:1')\n",
      "tensor([47.8355, 47.4898, 47.5243], device='cuda:1')\n",
      "tensor([47.5580, 47.3928, 47.4028], device='cuda:1')\n",
      "tensor([47.4066, 47.4149, 47.4372], device='cuda:1')\n",
      "tensor([47.4183, 47.8005, 47.2594], device='cuda:1')\n",
      "tensor([47.3364, 47.6234, 47.3921], device='cuda:1')\n",
      "tensor([47.4492, 47.5274, 47.5855], device='cuda:1')\n",
      "tensor([47.6550, 47.3172, 47.4792], device='cuda:1')\n",
      "tensor([47.3699, 47.6981, 47.5216], device='cuda:1')\n",
      "tensor([47.4867, 47.5093, 47.8707], device='cuda:1')\n",
      "tensor([47.4815, 47.7424, 47.6198], device='cuda:1')\n",
      "loss : 0.9057\n",
      "tensor([47.5830, 47.4409, 47.4995], device='cuda:1')\n",
      "tensor([47.5440, 47.5325, 47.4488], device='cuda:1')\n",
      "tensor([47.3687, 47.2552, 47.5463], device='cuda:1')\n",
      "tensor([47.5963, 47.5909, 47.6428], device='cuda:1')\n",
      "tensor([47.4019, 47.4862, 47.5203], device='cuda:1')\n",
      "tensor([47.3295, 47.5936, 47.4585], device='cuda:1')\n",
      "tensor([47.5709, 47.4102, 47.4871], device='cuda:1')\n",
      "tensor([47.5840, 47.6225, 47.6906], device='cuda:1')\n",
      "tensor([47.2228, 47.5289, 47.4076], device='cuda:1')\n",
      "tensor([47.7447, 47.4044, 47.4997], device='cuda:1')\n",
      "tensor([47.5664, 47.7775, 47.4321], device='cuda:1')\n",
      "tensor([47.3833, 47.5885, 47.4512], device='cuda:1')\n",
      "tensor([47.3522, 47.5811, 47.5740], device='cuda:1')\n",
      "tensor([47.4657, 47.1948, 47.3923], device='cuda:1')\n",
      "tensor([47.7253, 47.5510, 47.2784], device='cuda:1')\n",
      "tensor([47.6377, 47.5458, 47.5134], device='cuda:1')\n",
      "tensor([47.5194, 47.6284, 47.6649], device='cuda:1')\n",
      "tensor([47.4284, 47.5493, 47.5410], device='cuda:1')\n",
      "tensor([47.5564, 47.4716, 47.4153], device='cuda:1')\n",
      "tensor([47.5836, 47.4011, 47.5038], device='cuda:1')\n",
      "tensor([47.4839, 47.4755, 47.6090], device='cuda:1')\n",
      "tensor([47.4477, 47.3312, 47.5598], device='cuda:1')\n",
      "tensor([47.4788, 47.6481, 47.4982], device='cuda:1')\n",
      "tensor([47.5031, 47.4362, 47.3934], device='cuda:1')\n",
      "tensor([47.4117, 47.4195, 47.5173], device='cuda:1')\n",
      "tensor([47.4831, 47.6419, 47.6447], device='cuda:1')\n",
      "tensor([47.7757, 47.6335, 47.3771], device='cuda:1')\n",
      "tensor([47.4735, 47.4563, 47.3660], device='cuda:1')\n",
      "tensor([47.5081, 47.2083, 47.5227], device='cuda:1')\n",
      "tensor([47.2871, 47.8407, 47.2282], device='cuda:1')\n",
      "tensor([47.4195, 47.2682, 47.5247], device='cuda:1')\n",
      "tensor([47.6086, 47.5364, 47.3345], device='cuda:1')\n",
      "loss : 0.7885\n",
      "tensor([47.4695, 47.5706, 47.5089], device='cuda:1')\n",
      "tensor([47.4861, 47.5804, 47.4731], device='cuda:1')\n",
      "tensor([47.5114, 47.4920, 47.3813], device='cuda:1')\n",
      "tensor([47.3849, 47.6569, 47.3498], device='cuda:1')\n",
      "tensor([47.4312, 47.5733, 47.4599], device='cuda:1')\n",
      "tensor([47.3330, 47.4843, 47.4346], device='cuda:1')\n",
      "tensor([47.6566, 47.3770, 47.4881], device='cuda:1')\n",
      "tensor([47.4022, 47.5924, 47.5413], device='cuda:1')\n",
      "tensor([47.1377, 47.2668, 47.7837], device='cuda:1')\n",
      "tensor([47.5844, 47.5132, 47.5547], device='cuda:1')\n",
      "tensor([47.6419, 47.4331, 47.4277], device='cuda:1')\n",
      "tensor([47.7476, 47.2918, 47.5406], device='cuda:1')\n",
      "tensor([47.6299, 47.7522, 47.6179], device='cuda:1')\n",
      "tensor([47.5836, 47.5020, 47.5476], device='cuda:1')\n",
      "tensor([47.6603, 47.4637, 47.5946], device='cuda:1')\n",
      "tensor([47.7036, 47.1659, 47.5401], device='cuda:1')\n",
      "tensor([47.4846, 47.3984, 47.4983], device='cuda:1')\n",
      "tensor([47.4595, 47.6138, 47.6276], device='cuda:1')\n",
      "tensor([47.4657, 47.4836, 47.3882], device='cuda:1')\n",
      "tensor([47.5425, 47.4955, 47.6154], device='cuda:1')\n",
      "tensor([47.5557, 47.4014, 47.3839], device='cuda:1')\n",
      "tensor([47.3438, 47.5103, 47.4596], device='cuda:1')\n",
      "tensor([47.5964, 47.4426, 47.5249], device='cuda:1')\n",
      "tensor([47.3959, 47.7025, 47.3939], device='cuda:1')\n",
      "tensor([47.4808, 47.5199, 47.4585], device='cuda:1')\n",
      "tensor([47.3235, 47.6341, 47.7714], device='cuda:1')\n",
      "tensor([47.8418, 47.4495, 47.4630], device='cuda:1')\n",
      "tensor([47.4639, 47.4360, 47.3284], device='cuda:1')\n",
      "tensor([47.5713, 47.2389, 47.3713], device='cuda:1')\n",
      "tensor([47.7946, 47.9678, 47.5099], device='cuda:1')\n",
      "tensor([47.2410, 47.1423, 47.6134], device='cuda:1')\n",
      "tensor([47.5351, 47.5705, 47.6048], device='cuda:1')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m16\u001b[39m, (\u001b[39m1\u001b[39m, num_classes, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, num_classes, (\u001b[39m1\u001b[39m, num_classes, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m l \u001b[39m=\u001b[39m loss(probs, labels)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss : \u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/medsegdiff/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 21\u001b[0m, in \u001b[0;36mMultiNeighborLoss.forward\u001b[0;34m(self, probs, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m delta \u001b[39m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(probs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)):\n\u001b[0;32m---> 21\u001b[0m     p_angles, l_angles \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_angles(torch\u001b[39m.\u001b[39msigmoid(probs[i, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m])), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_angles(labels[i, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m])\n\u001b[1;32m     22\u001b[0m     delta\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39msquare(p_angles \u001b[39m-\u001b[39m l_angles))\n\u001b[1;32m     24\u001b[0m delta \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(delta)\n",
      "Cell \u001b[0;32mIn[110], line 55\u001b[0m, in \u001b[0;36mMultiNeighborLoss.compute_angles\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_count):\n\u001b[1;32m     54\u001b[0m     n \u001b[39m=\u001b[39m vectors[j]\n\u001b[0;32m---> 55\u001b[0m     angle \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39macos(torch\u001b[39m.\u001b[39;49mdot(m, n) \u001b[39m/\u001b[39;49m (torch\u001b[39m.\u001b[39;49mnorm(m) \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mnorm(n)))\n\u001b[1;32m     56\u001b[0m     angles[idx] \u001b[39m=\u001b[39m angle\n\u001b[1;32m     57\u001b[0m     idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = 16\n",
    "device = torch.device(\"cuda:1\")\n",
    "loss = MultiNeighborLoss(num_classes)\n",
    "for _ in range(100):\n",
    "    probs = torch.randint(0, 16, (1, num_classes, 96, 96, 96)).to(device)\n",
    "    labels = torch.randint(0, num_classes, (1, num_classes, 96, 96, 96)).to(device)\n",
    "    \n",
    "    l = loss(probs, labels)\n",
    "    \n",
    "    print(f\"loss : {l:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionUnet(\n",
      "  (model): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Convolution(\n",
      "          (conv): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): AttentionLayer(\n",
      "      (attention): AttentionBlock(\n",
      "        (W_g): Sequential(\n",
      "          (0): Convolution(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (W_x): Sequential(\n",
      "          (0): Convolution(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (psi): Sequential(\n",
      "          (0): Convolution(\n",
      "            (conv): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "          (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Sigmoid()\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (upconv): UpConv(\n",
      "        (up): Convolution(\n",
      "          (conv): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (D): Dropout(p=0.0, inplace=False)\n",
      "            (A): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merge): Convolution(\n",
      "        (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (adn): ADN(\n",
      "          (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          (D): Dropout(p=0.0, inplace=False)\n",
      "          (A): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (submodule): Sequential(\n",
      "        (0): ConvBlock(\n",
      "          (conv): Sequential(\n",
      "            (0): Convolution(\n",
      "              (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): ReLU()\n",
      "              )\n",
      "            )\n",
      "            (1): Convolution(\n",
      "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): ReLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): AttentionLayer(\n",
      "          (attention): AttentionBlock(\n",
      "            (W_g): Sequential(\n",
      "              (0): Convolution(\n",
      "                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "              (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (W_x): Sequential(\n",
      "              (0): Convolution(\n",
      "                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "              (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (psi): Sequential(\n",
      "              (0): Convolution(\n",
      "                (conv): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "              (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): Sigmoid()\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (upconv): UpConv(\n",
      "            (up): Convolution(\n",
      "              (conv): ConvTranspose3d(256, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (D): Dropout(p=0.0, inplace=False)\n",
      "                (A): ReLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (merge): Convolution(\n",
      "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (adn): ADN(\n",
      "              (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "              (D): Dropout(p=0.0, inplace=False)\n",
      "              (A): PReLU(num_parameters=1)\n",
      "            )\n",
      "          )\n",
      "          (submodule): Sequential(\n",
      "            (0): ConvBlock(\n",
      "              (conv): Sequential(\n",
      "                (0): Convolution(\n",
      "                  (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (1): Convolution(\n",
      "                  (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): ReLU()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): AttentionLayer(\n",
      "              (attention): AttentionBlock(\n",
      "                (W_g): Sequential(\n",
      "                  (0): Convolution(\n",
      "                    (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                  )\n",
      "                  (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (W_x): Sequential(\n",
      "                  (0): Convolution(\n",
      "                    (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                  )\n",
      "                  (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (psi): Sequential(\n",
      "                  (0): Convolution(\n",
      "                    (conv): Conv3d(128, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                  )\n",
      "                  (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (2): Sigmoid()\n",
      "                )\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (upconv): UpConv(\n",
      "                (up): Convolution(\n",
      "                  (conv): ConvTranspose3d(512, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    (D): Dropout(p=0.0, inplace=False)\n",
      "                    (A): ReLU()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (merge): Convolution(\n",
      "                (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                (adn): ADN(\n",
      "                  (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                  (D): Dropout(p=0.0, inplace=False)\n",
      "                  (A): PReLU(num_parameters=1)\n",
      "                )\n",
      "              )\n",
      "              (submodule): Sequential(\n",
      "                (0): ConvBlock(\n",
      "                  (conv): Sequential(\n",
      "                    (0): Convolution(\n",
      "                      (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                      (adn): ADN(\n",
      "                        (N): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                        (D): Dropout(p=0.0, inplace=False)\n",
      "                        (A): ReLU()\n",
      "                      )\n",
      "                    )\n",
      "                    (1): Convolution(\n",
      "                      (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                      (adn): ADN(\n",
      "                        (N): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                        (D): Dropout(p=0.0, inplace=False)\n",
      "                        (A): ReLU()\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (1): AttentionLayer(\n",
      "                  (attention): AttentionBlock(\n",
      "                    (W_g): Sequential(\n",
      "                      (0): Convolution(\n",
      "                        (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                      )\n",
      "                      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    )\n",
      "                    (W_x): Sequential(\n",
      "                      (0): Convolution(\n",
      "                        (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                      )\n",
      "                      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    )\n",
      "                    (psi): Sequential(\n",
      "                      (0): Convolution(\n",
      "                        (conv): Conv3d(256, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                      )\n",
      "                      (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (2): Sigmoid()\n",
      "                    )\n",
      "                    (relu): ReLU()\n",
      "                  )\n",
      "                  (upconv): UpConv(\n",
      "                    (up): Convolution(\n",
      "                      (conv): ConvTranspose3d(1024, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                      (adn): ADN(\n",
      "                        (N): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                        (D): Dropout(p=0.0, inplace=False)\n",
      "                        (A): ReLU()\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (merge): Convolution(\n",
      "                    (conv): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.0, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                  (submodule): ConvBlock(\n",
      "                    (conv): Sequential(\n",
      "                      (0): Convolution(\n",
      "                        (conv): Conv3d(512, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                        (adn): ADN(\n",
      "                          (N): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (D): Dropout(p=0.0, inplace=False)\n",
      "                          (A): ReLU()\n",
      "                        )\n",
      "                      )\n",
      "                      (1): Convolution(\n",
      "                        (conv): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                        (adn): ADN(\n",
      "                          (N): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (D): Dropout(p=0.0, inplace=False)\n",
      "                          (A): ReLU()\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Convolution(\n",
      "      (conv): Conv3d(64, 13, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets.attentionunet import AttentionUnet\n",
    "\n",
    "model = AttentionUnet(3, 1, 13, [64, 128, 256, 512, 1024], [2, 2, 2, 2, 2])\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattention_diff_unet\u001b[39;00m \u001b[39mimport\u001b[39;00m AttentionDiffUNet\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m AttentionDiffUNet(\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m13\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m))\n\u001b[1;32m      5\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1000\u001b[39m, (\u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/ws/diff-unet-easy/models/attention_diff_unet.py:28\u001b[0m, in \u001b[0;36mAttentionDiffUNet.__init__\u001b[0;34m(self, spatial_dims, in_channels, out_channels, features, kernel_size, stride, pool_size, dropout, timesteps, mode)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m out_channels\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n\u001b[0;32m---> 28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_model \u001b[39m=\u001b[39m AttentionUNetEncoder(spatial_dims, in_channels, out_channels, features)    \n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AttentionUNetDecoder(spatial_dims, in_channels, out_channels, features\u001b[39m=\u001b[39mfeatures)    \n\u001b[1;32m     31\u001b[0m betas \u001b[39m=\u001b[39m get_named_beta_schedule(\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m, timesteps)\n",
      "File \u001b[0;32m~/ws/diff-unet-easy/models/attention_unet.py:292\u001b[0m, in \u001b[0;36mAttentionUNetEncoder.__init__\u001b[0;34m(self, spatial_dims, in_channels, features, pool_size, dropout)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m    286\u001b[0m              spatial_dims: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[1;32m    287\u001b[0m              in_channels: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, \n\u001b[1;32m    288\u001b[0m              features: Sequence[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m1024\u001b[39m],\n\u001b[1;32m    289\u001b[0m              pool_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m    290\u001b[0m              dropout: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead \u001b[39m=\u001b[39m Conv(spatial_dims, in_channels, features[\u001b[39m0\u001b[39;49m], dropout)\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[1;32m    295\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(features[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from models.attention_diff_unet import AttentionDiffUNet\n",
    "\n",
    "model = AttentionDiffUNet(3, 3, 13)\n",
    "x = torch.randn((4, 3, 96, 96, 96))\n",
    "image = torch.randint(0, 1000, (4, 1))\n",
    "label = torch.randn((4, 3, 96, 96, 96))\n",
    "x_start = (label) * 2 - 1\n",
    "x_t, t, _ = model(x=x_start, pred_type=\"q_sample\")\n",
    "pred = model(x=x_t, step=t, image=image, pred_type=\"denoise\")\n",
    "\n",
    "print(f\"x_t : {x_t.shape}\")\n",
    "print(f\"t : {t.shape}\")\n",
    "print(f\"pred : {pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65603514 0.47183063]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "dices = \"logs/diff-unet-msd-1/dices.pkl\"\n",
    "\n",
    "with open(dices, 'rb') as file:\n",
    "    dices = pickle.load(file)\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "scores = np.zeros((len(dices), num_classes))\n",
    "for i, dice in enumerate(dices):\n",
    "    dice = list(dice.values())\n",
    "    scores[i, 0] = dice[0]\n",
    "    scores[i, 1] = dice[1]\n",
    "    \n",
    "print(np.mean(scores, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "dices = \"logs/diff-unet-btcv-29/dices.pkl\"\n",
    "\n",
    "with open(dices, 'rb') as file:\n",
    "    dices = pickle.load(file)\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "scores = np.zeros((len(dices), num_classes))\n",
    "for i, dice in enumerate(dices):\n",
    "    dice = list(dice.values())\n",
    "    scores[i, 0] = dice[0]\n",
    "    scores[i, 1] = dice[1]\n",
    "    \n",
    "print(np.mean(scores, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
      "sizeof_hdr      : 348\n",
      "data_type       : b''\n",
      "db_name         : b''\n",
      "extents         : 0\n",
      "session_error   : 0\n",
      "regular         : b'r'\n",
      "dim_info        : 0\n",
      "dim             : [  3 768 768  90   1   1   1   1]\n",
      "intent_p1       : 0.0\n",
      "intent_p2       : 0.0\n",
      "intent_p3       : 0.0\n",
      "intent_code     : none\n",
      "datatype        : int16\n",
      "bitpix          : 16\n",
      "slice_start     : 0\n",
      "pixdim          : [-1.         0.5703125  0.5703125  5.         0.         0.\n",
      "  0.         0.       ]\n",
      "vox_offset      : 0.0\n",
      "scl_slope       : nan\n",
      "scl_inter       : nan\n",
      "slice_end       : 0\n",
      "slice_code      : unknown\n",
      "xyzt_units      : 10\n",
      "cal_max         : 0.0\n",
      "cal_min         : 0.0\n",
      "slice_duration  : 0.0\n",
      "toffset         : 0.0\n",
      "glmax           : 0\n",
      "glmin           : 0\n",
      "descrip         : b'Time=144225.000'\n",
      "aux_file        : b''\n",
      "qform_code      : scanner\n",
      "sform_code      : scanner\n",
      "quatern_b       : 0.0\n",
      "quatern_c       : 1.0\n",
      "quatern_d       : 0.0\n",
      "qoffset_x       : 233.0\n",
      "qoffset_y       : -373.4297\n",
      "qoffset_z       : 26.5\n",
      "srow_x          : [ -0.5703125  -0.          0.        233.       ]\n",
      "srow_y          : [   0.           0.5703125    0.        -373.4297   ]\n",
      "srow_z          : [ 0.   0.   5.  26.5]\n",
      "intent_name     : b''\n",
      "magic           : b'n+1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import nibabel\n",
    "\n",
    "data_path = \"/home/song99/ws/datasets/AMOS\"\n",
    "images = glob.glob(os.path.join(data_path, \"imagesTr/*.nii.gz\"))\n",
    "\n",
    "for image in images:\n",
    "    img = nibabel.load(image)\n",
    "    print(img.header)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
      "sizeof_hdr      : 348\n",
      "data_type       : b''\n",
      "db_name         : b''\n",
      "extents         : 0\n",
      "session_error   : 0\n",
      "regular         : b'r'\n",
      "dim_info        : 0\n",
      "dim             : [  3 512 512 117   1   1   1   1]\n",
      "intent_p1       : 0.0\n",
      "intent_p2       : 0.0\n",
      "intent_p3       : 0.0\n",
      "intent_code     : none\n",
      "datatype        : int16\n",
      "bitpix          : 16\n",
      "slice_start     : 0\n",
      "pixdim          : [1.      0.90625 0.90625 3.      0.      0.      0.      0.     ]\n",
      "vox_offset      : 0.0\n",
      "scl_slope       : nan\n",
      "scl_inter       : nan\n",
      "slice_end       : 0\n",
      "slice_code      : unknown\n",
      "xyzt_units      : 10\n",
      "cal_max         : 0.0\n",
      "cal_min         : 0.0\n",
      "slice_duration  : 0.0\n",
      "toffset         : 0.0\n",
      "glmax           : 2861\n",
      "glmin           : -1024\n",
      "descrip         : b''\n",
      "aux_file        : b''\n",
      "qform_code      : unknown\n",
      "sform_code      : unknown\n",
      "quatern_b       : 0.0\n",
      "quatern_c       : 0.0\n",
      "quatern_d       : 0.0\n",
      "qoffset_x       : 0.0\n",
      "qoffset_y       : 0.0\n",
      "qoffset_z       : 0.0\n",
      "srow_x          : [0. 0. 0. 0.]\n",
      "srow_y          : [0. 0. 0. 0.]\n",
      "srow_z          : [0. 0. 0. 0.]\n",
      "intent_name     : b''\n",
      "magic           : b'n+1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import nibabel\n",
    "\n",
    "data_path = \"/home/song99/ws/datasets/BTCV\"\n",
    "images = glob.glob(os.path.join(data_path, \"imagesTr/*.nii.gz\"))\n",
    "\n",
    "for image in images:\n",
    "    img = nibabel.load(image)\n",
    "    print(img.header)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " (96, 96, 96))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def window_partition_5d(x: torch.Tensor, window_size: int):\n",
    "    \"\"\"\n",
    "    Partition into non-overlapping windows with padding if needed.\n",
    "    Args:\n",
    "        x (tensor): input tokens with [B, D, H, W, C].\n",
    "        window_size (int): window size.\n",
    "\n",
    "    Returns:\n",
    "        windows: windows after partition with [B * D * num_windows, window_size, window_size, C].\n",
    "        (Dp, Hp, Wp): padded depth, height, and width before partition\n",
    "    \"\"\"\n",
    "    B, D, H, W, C = x.shape\n",
    "\n",
    "    pad_d = (window_size - D % window_size) % window_size\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, 0, 0, pad_w, 0, pad_h, 0, pad_d))\n",
    "    Dp, Hp, Wp = D + pad_d, H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Dp // window_size, window_size, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 5, 4, 6, 7).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows, (Dp, Hp, Wp)\n",
    "\n",
    "x = torch.zeros((1, 96, 96, 96, 768))\n",
    "\n",
    "window_partition_5d(x, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected dilation to be a single integer value or a list of 2 values to match the convolution dimensions, but got dilation=[1, 1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m AttentionUNet()\n\u001b[1;32m      5\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m out \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/medsegdiff/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ws/diff-unet-easy/models/attention_unet/attention_unet.py:91\u001b[0m, in \u001b[0;36mAttentionUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 91\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embed(x)\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n",
      "File \u001b[0;32m~/anaconda3/envs/medsegdiff/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ws/diff-unet-easy/models/attention_unet/attention_unet.py:141\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(x)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    144\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# B C H W -> B H W C\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medsegdiff/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/medsegdiff/lib/python3.8/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/medsegdiff/lib/python3.8/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[1;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    610\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected dilation to be a single integer value or a list of 2 values to match the convolution dimensions, but got dilation=[1, 1, 1]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.attention_unet import AttentionUNet\n",
    "\n",
    "model = AttentionUNet()\n",
    "x = torch.ones((1, 1, 96, 96, 96))\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
